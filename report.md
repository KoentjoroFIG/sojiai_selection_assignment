Approach:
The system I built basically follows a three-stage pipeline. first is text extraction, second is AD extraction using an LLM with structured output, and and third is applicability evaluation using both rule-based and LLM-assisted methods.

The first stage is text extrcation, which reads the PDF and converts it into raw text. Because the provided AD documents were standard digital PDFs and not scanned images, I used pdfplumber. It is lightweight and reliable for this case since the text is readable by machine. But if in the future we will have scanned images pdf than we will need to implement OCR extraction. To make the current design flexible and future proof, I develop this in a using Strategy Pattern so that a different extractor, such as an OCR, can be introduced later without modifying the rest of the pipeline. Although I did not implement OCR support yet, I left a placeholder class for it.

The second stage is AD extraction. In this stage, the raw text output from the PDF is converted into structured data using GPT-4o. Instead of writing a custom parser filled with rules or regex patterns, I relied on the LLM to interpret the content and return JSON that matches a Pydantic schema. I used the structured response format so the model is required to retrun valid JSON that matches the schema. This ensures consistency and removes ambiguity in the output. This approach is far more robust than rule-based parsing because different aviation authorities use very different formats and style of writing. Future documents from other authorities will likely introduce additional variation. The LLM is able to generalize across all of these differences while still producing structured data. It will also be able to handle complex language and edge cases that would be difficult to capture with rules alone. For robustness, I put a field in the schema to capture additional rules that my be introduced in the furute AD documents. So we don't need to change the schema every time a new field is added.

The final stage of the pipeline is evaluation. I implemented two evaluation paths so the system can handle both structured and unstructured user inputs. When structured JSON is provided, the system uses a deterministic rule-based evaluator. This evaluator checks the aircraft model and its variants, applies MSN range logic, and evaluates modification exemptions using fuzzy matching to compensate for inconsistent naming across documents. When the user provides a free-text description of an aircraft configuration, the system uses an LLM-assisted evaluation flow. In this case, the LLM interprets the unstructured description and generates natural language response that answer the request. With this approach, I am able to show you both a reliable rule-based evaluation and a more flexible LLM-assisted evaluation.

Chalange:
Developing the rule-based evaluator was the most challenging part of the project because applicability rules involve many edge cases. Model variants follow inconsistent naming conventions, modifications can appear in multiple forms, and MSN logic sometimes includes complex combinations of ranges and exceptions. To speed up the development process, I used github copilot as a coding assistant. I prompted it to generate preliminary logic, which I refined and validated through testing. And here is the only place I use vibe code for this project since it will be hard to make it our self without spending a lot of time.

Another challenge was choosing how to present the assignment because no specific output format or interface was required. I decided to build a FastAPI service because it allows easy testing through HTTP calls and provides a clean structure for separating stages of the pipeline.

Managing LLM hallucination was also important. I reduced this risk by enforcing strict schema validation, using low temperature settings, and relying on the structured output format to keep the model controlled.

Limitation:
There are several limitations in the current system. First, because pdfplumber only supports text-based PDFs, the pipeline does not work on scanned documents. Integrating an OCR extractor would be the next step to address this limitation.

Second, the rule-based evaluator handles current logic well, but it may struggle with more complex applicability conditions that involve complex constraints in the future. The LLM-assisted evaluator can interpret these cases more easily, but the concern is in cost and latency. But, I think it can be ignored since the user is might be big company and not be used too frequent. GPT-4o provides excellent accuracy, but if possible I would like to explore fine-tuning a smaller model to reduce costs and make the model is field-specific to make it more reliable.

Tread-offs:
I chose an LLM for extraction because it saves a huge amount of engineering effort. If I tried to write a rule-based extractor for every authorityâ€™s formatting style, it would be fragile and probably break every time a new AD is published. The LLM is more flexible. The trade-off is that it is more expensive and have a black box uncertainty behind it that may be difficult to fully understand how the AI decide it. But, I minimise it by I using strict schema structure and clear system instruction. And the fact that volume of ADs is not huge in real life, it can be acceptable.

I also did not use any vision-language model because these ADs are text-based. There is no benefit in involving a VLM, it is more expensive and have longer latency. However, if future ADs include diagrams or images, then a VLM might become useful. So for now it is better to use text extraction and pass the extracted text to the chat-completion LLM.
